import datetime as dt
from datetime import datetime,timedelta
import sys
from awsglue.transforms import *
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.dynamicframe import DynamicFrame
from awsglue.job import Job
from pyspark.sql.functions import *
from pyspark.sql.types import *
from pyspark.sql import SparkSession,SQLContext, Row
from pyspark.sql.functions import col,when,col,concat,split
from pyspark.sql import functions as sf
from modules import *
from botocore.exceptions import ClientError
import logging
from pyspark.sql.types import NullType
import pyspark.sql.functions as F


def main():
    # global partition_year,partition_Month,query,busdate,args,sc,glueContext,spark,job,sys,secretName,rs_jdbc,rs_username,rs_pwd,dfsource,dftrans,bucket_name,prefix,outpath,outputfilename,con_params
    try:
        # --------------------Initial Function to setup environment and logging--------------------------------
        settingUpEnvironment()

        # --------------------Getting Current Month date------------------------------------------------
        rundate=args['rundate']
        year,month,day = processRunDate(rundate)

        filerundate1=datetime.strftime(datetime.strptime(rundate, "%Y-%m-%d")-timedelta(days=1),"%Y-%m-%d")
        print("Determined first file of the week:" + filerundate1+"\n")
        filerundate2=datetime.strftime(datetime.strptime(rundate, "%Y-%m-%d")-timedelta(days=2),"%Y-%m-%d")
        print("Determined second file of the week:" + filerundate2+"\n")
        filerundate3=datetime.strftime(datetime.strptime(rundate, "%Y-%m-%d")-timedelta(days=3),"%Y-%m-%d")
        print("Determined third file of the week:" + filerundate3+"\n")
        filerundate4=datetime.strftime(datetime.strptime(rundate, "%Y-%m-%d")-timedelta(days=4),"%Y-%m-%d")
        print("Determined fourth file of the week:" + filerundate4+"\n")
        filerundate5=datetime.strftime(datetime.strptime(rundate, "%Y-%m-%d")-timedelta(days=5),"%Y-%m-%d")
        print("Determined fifth file of the week:" + filerundate5+"\n")
        filerundate6=datetime.strftime(datetime.strptime(rundate, "%Y-%m-%d")-timedelta(days=6),"%Y-%m-%d")
        print("Determined sixth file of the week:" + filerundate6+"\n")
        filerundate7=datetime.strftime(datetime.strptime(rundate, "%Y-%m-%d")-timedelta(days=7),"%Y-%m-%d")
        print("Determined seventh file of the week:" + filerundate7+"\n")


        #--------------------Below method is to read data from Athena-----------------------------------
        databaseName=args['athenaDatabase']
        tableName=args['athenaTableEtlDemo']
        
        partition_year1,partition_Month1,partition_Day1=processRunDate(filerundate1)
        partition_year2,partition_Month2,partition_Day2=processRunDate(filerundate2)
        partition_year3,partition_Month3,partition_Day3=processRunDate(filerundate3)
        partition_year4,partition_Month4,partition_Day4=processRunDate(filerundate4)
        partition_year5,partition_Month5,partition_Day5=processRunDate(filerundate5)
        partition_year6,partition_Month6,partition_Day6=processRunDate(filerundate6)
        partition_year7,partition_Month7,partition_Day7=processRunDate(filerundate7)
        
        #--------------------Below method is for Reading from Source- Athena -------------------------------
        dfsource1=read_source_data_athena(sc,databaseName,tableName,partition_year1,partition_Month1,partition_Day1)
        dfsource2=read_source_data_athena(sc,databaseName,tableName,partition_year2,partition_Month2,partition_Day2)
        dfsource3=read_source_data_athena(sc,databaseName,tableName,partition_year3,partition_Month3,partition_Day3)
        dfsource4=read_source_data_athena(sc,databaseName,tableName,partition_year4,partition_Month4,partition_Day4)
        dfsource5=read_source_data_athena(sc,databaseName,tableName,partition_year5,partition_Month5,partition_Day5)
        dfsource6=read_source_data_athena(sc,databaseName,tableName,partition_year6,partition_Month6,partition_Day6)
        dfsource7=read_source_data_athena(sc,databaseName,tableName,partition_year7,partition_Month7,partition_Day7)

        #--------------------Below method is for applying transformation logic -------------------------------
        dftrans=transformation_logic(rundate,dfsource1,dfsource2,dfsource3,dfsource4,dfsource5,dfsource6,dfsource7,filerundate1,filerundate2,filerundate3,filerundate4,filerundate5,filerundate6,filerundate7,year,month,day)

        print("Data Transformation completed")

        #--------------------Below method is to write data to S3----------------------------------------

        bucket_name = args['bucketName']
        prefixparquet = args['parquetfilepath']
        outpath = "s3://" + bucket_name + "/" + prefixparquet
        print("outpath:", outpath)
        fileFormat = "parquet"

        partitionkeyslist = ['Year', 'Month', 'Day']

        #-----------------------Write output file as a parquet format to S3-----------------------
        write_data_parquet(sc, dftrans, outpath, fileFormat, partitionkeyslist)

        # --------------------Below method is for renaming parquet file to required outputfilename----------------
        outputfilename = args['outputfilename'] + ".parquet"
        print("outputfilename:", outputfilename)
        prefix = prefixparquet + "/Year=" + year + "/Month=" + month +  "/Day=" + day + "/"
        print("prefix:", prefix)
        print("outputfilename:   s3://" + bucket_name + "/" + prefix + "/" + outputfilename)
        rename_file(bucket_name, prefix, outputfilename)
 
        # -----------------------Writing data as text format for comparision--------------------------------------
        prefixtext = args['textfilepath']
        fileFormat = "csv"
        outpath = "s3://" + bucket_name + "/" + prefixtext
        print("textoutpath : s3://" + bucket_name + "/" + prefixtext)
        write_data_csv_with_seperator_tab(sc, dftrans, outpath, fileFormat, partitionkeyslist)
        
        # --------------------Below method is for renaming text file to required outputfilename----------------
        outputfilename = args['outputfilename'] + ".TXT"
        prefix = prefixtext + "/Year=" + year + "/Month=" + month + "/Day=" + day + "/"
        print("textfilename:   s3://" + bucket_name + "/" + prefix + "/" + outputfilename)
        rename_file(bucket_name, prefix, outputfilename)
        
        
        print("job completed")
        job.commit()

    except Exception as excptn:
        print("Error in MAIN function: {0}".format(excptn))
        sys.exit(1)


def settingUpEnvironment():
    print("---------------Setting Up Envrionment----------------\n")
    global args, sc, glueContext, spark, logger, job
    try:
        args = getResolvedOptions(sys.argv, ['JOB_NAME', 'rundate', 'bucketName', 'athenaDatabase', 'athenaTableEtlDemo',
                                             'outputfilename', 'textfilepath', 'parquetfilepath'])
        sc = SparkContext()
        glueContext = GlueContext(sc)
        spark = glueContext.spark_session
        log4jLogger = spark.sparkContext._jvm.org.apache.log4j
        logger = log4jLogger.LogManager.getLogger(__name__)
        job = Job(glueContext)
        job.init(args['JOB_NAME'], args)
    except Exception as excptn:
        print("Error in settingUpEnvironment function: {0}".format(excptn))
        sys.exit(1)


def handlezerobytedataframes(dynamicframe):
    try:
        selectfields_df_source = SelectFields.apply(frame = dynamicframe, paths = ["office1","acctn","loandate","ltv","atp","app_state","app_branch","homecode","riskrank","fico","srcexp","cobr_ind","auto_ind","rbo"],transformation_ctx = "selectfields_df_source")                                                                  
        dfsource = selectfields_df_source.toDF()
        
        schema =    StructType([    
                    StructField('office1', StringType(), True), 
                    StructField('acctn', StringType(), True), 
                    StructField('loandate', StringType(), True),
                    StructField('ltv', StringType(), True),
                    StructField('atp', StringType(), True),
                    StructField('app_state', StringType(), True),
                    StructField('app_branch', StringType(), True),
                    StructField('homecode', StringType(), True),
                    StructField('riskrank', StringType(), True),
                    StructField('fico', StringType(), True), 
                    StructField('srcexp', StringType(), True),
                    StructField('cobr_ind', StringType(), True),
                    StructField('auto_ind', StringType(), True),
                    StructField('rbo', StringType(), True) ])
        
        if (len(dfsource.head(1)) > 0):
            dfsource = dfsource
        else:
            print("Zero record for the dfsource,creating empty dataframe")
            dfsource = spark.createDataFrame([], schema)
        
        return dfsource

    except Exception as e:
        print("Error in processDate : ", e)
        sys.exit(1)
        

def transformation_logic(rundate,dfsource1,dfsource2,dfsource3,dfsource4,dfsource5,dfsource6,dfsource7,filerundate1,filerundate2,filerundate3,filerundate4,filerundate5,filerundate6,filerundate7,year,month,day):
    try:
        #--------------------Creating Dataframe-------------------------------------------------------------
        dfsource1_filter=handlezerobytedataframes(dfsource1)
        dfsource2_filter=handlezerobytedataframes(dfsource2)
        dfsource3_filter=handlezerobytedataframes(dfsource3)
        dfsource4_filter=handlezerobytedataframes(dfsource4)
        dfsource5_filter=handlezerobytedataframes(dfsource5)
        dfsource6_filter=handlezerobytedataframes(dfsource6)
        dfsource7_filter=handlezerobytedataframes(dfsource7)
        
        dfsource1_filter = dfsource1_filter.withColumn('asof',lit(filerundate1))
        dfsource2_filter = dfsource2_filter.withColumn('asof',lit(filerundate2))
        dfsource3_filter = dfsource3_filter.withColumn('asof',lit(filerundate3))
        dfsource4_filter = dfsource4_filter.withColumn('asof',lit(filerundate4))
        dfsource5_filter = dfsource5_filter.withColumn('asof',lit(filerundate5))
        dfsource6_filter = dfsource6_filter.withColumn('asof',lit(filerundate6))
        dfsource7_filter = dfsource7_filter.withColumn('asof',lit(filerundate7))
        
        
        #--------------------Applying transformation logic--------------------------------
        union_df = dfsource1_filter.union(dfsource2_filter).union(dfsource3_filter).union(dfsource4_filter).union(dfsource5_filter).union(dfsource6_filter).union(dfsource7_filter)
        union_source_df = union_df
        union_df = union_df.withColumn('office2',sf.concat(col('app_state'),col('app_branch')))
        union_df = union_df.withColumn('office',when(col('app_State') >= 0,col('office2')).otherwise(col('office1')))
        union_df = union_df.filter(union_df.acctn > 0)
        union_df = union_df.withColumn('note_dy',date_format(to_date(union_df.loandate,'yyyyMMdd'),'MM/dd/yyyy'))
        union_df = union_df.withColumn('asof',date_format(to_date(union_df.asof,'yyyy-MM-dd'),'MM/dd/yyyy'))
        union_df = union_df.withColumn('auto_ind',when(col('auto_ind') != 'Y' ,lit('N')).otherwise(col('auto_ind')))
        union_df = union_df.withColumn('coborrower',when(col('cobr_ind') == 'Y' ,lit('1')).otherwise(lit('0')))
        union_df = union_df.withColumn('rbo',when(col('rbo') != 1 ,lit('0')).otherwise(col('rbo')))
        union_df = union_df.withColumn('fico',when(((union_df.fico.isNull()) | (col('fico') == "")) ,lit('0')).otherwise(coalesce(col('fico'),lit('0')).cast('integer')))
        union_df = union_df.withColumn('ltv',when(((union_df.ltv.isNull()) | (col('ltv') == "")) ,lit('0')).otherwise(coalesce(F.round(substring('ltv', 1,4)/10,0),lit('0')).cast('integer')))
        union_df = union_df.withColumn('atp',when(((union_df.atp.isNull()) | (col('atp') == "")) ,lit('0')).otherwise(coalesce(col('atp'),lit('0')).cast('integer')))
        union_df = union_df.withColumn('riskrank',when(trim(col('riskrank')) == 'EXCELLENT',trim(col('riskrank')))
                                                 .when(trim(col('riskrank')) == 'GOOD',trim(col('riskrank')))
                                                 .when(trim(col('riskrank')) == 'AVERAGE',trim(col('riskrank')))
                                                 .when(trim(col('riskrank')) == 'WEAK',trim(col('riskrank')))
                                                 .when(trim(col('riskrank')) == 'FAIL',trim(col('riskrank'))).otherwise(lit('UNKNOWN')))
        union_df = union_df.withColumn('riskcode',when(trim(col('riskrank')) == 'EXCELLENT',lit('5'))
                                                 .when(trim(col('riskrank')) == 'GOOD',lit('4'))
                                                 .when(trim(col('riskrank')) == 'AVERAGE', lit('3'))
                                                 .when(trim(col('riskrank')) == 'WEAK',lit('2'))
                                                 .when(trim(col('riskrank')) == 'FAIL', lit('1')).otherwise(lit('0')))
        
        union_df = union_df.withColumn('homeowner',when((((to_date(col('note_dy'),"MM/dd/yyyy") >= '08/01/2004' ) & (col('homecode').isin('02','03','06','08','10','11','12','13','14','15'))) | ((to_date(col('note_dy'),"MM/dd/yyyy") >= '06/01/2000' ) & (col('homecode').isin('2','3','6','8'))) | ((to_date(col('note_dy'),"MM/dd/yyyy") < '06/01/2000' ) & (col('homecode').isin('2','3','6','7','8')))),lit('1')).otherwise(lit('0')))
        union_df = union_df.orderBy(["office","acctn"], ascending=[0,0,0])
        
        union_df = union_df.dropDuplicates(["acctn"])
        union_df = union_df.withColumn('fiscal_year',substring('note_dy', 7,4))\
                           .withColumn('accounting_period',substring('note_dy',1,2))
                  
        union_df = union_df.withColumnRenamed('office','branch')\
                           .withColumnRenamed('acctn','account')\
                           .withColumnRenamed('note_dy','note_dt')\
                           .withColumnRenamed('fico','fico_original')\
                           .withColumnRenamed('ltv','ltv_original')\
                           .withColumnRenamed('riskrank','risk_rank_original')\
                           .withColumnRenamed('riskcode','risk_code_original')\
                           .withColumnRenamed('atp','ability_to_pay_original')\
                           .withColumnRenamed('srcexp','source_expanded')
               
        union_df = union_df.select("fiscal_year","accounting_period","branch","account","asof","homeowner","note_dt","fico_original","ltv_original","risk_rank_original","risk_code_original","ability_to_pay_original","rbo","auto_ind","source_expanded")                  
        union_df = union_df.withColumn('Year',lit(year))\
                        .withColumn('Month',lit(month))\
                        .withColumn('Day',lit(day))

        print("\n---------------Finished applying Transformation Logic-----------------------------\n") 
        print("\n---------------Total Number of records going to output dataset="+str(union_df.count())+"\n")
        
        DynamicF = DynamicFrame.fromDF(union_df,glueContext,"df_source") 
        DynamicF=DynamicF.repartition(1)
        
        return DynamicF
    except Exception as e:
        print("Error in Transformation : ", e)
        sys.exit(1)


if __name__ == "__main__":
    main()
